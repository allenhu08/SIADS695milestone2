{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "[Supervised - basic models](./supervised_basic.ipynb)\n",
    "\n",
    "[Supervised - RNN models](./supervised_rnn.ipynb)\n",
    "\n",
    "[Unsupervised - Word2Vec](./unsupervised_w2v.ipynb)\n",
    "\n",
    "[Unsupervised - Dimensionality Reduction](./unsupervised_dim.ipynb)\n",
    "\n",
    "[Unsupervised - LDA](./unsupervised_LDA.ipynb)\n",
    "\n",
    "[Performance Evaluation](./evaluation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\allenhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\allenhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\allenhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\allenhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "from torchtext.legacy.data import Field, LabelField, TabularDataset, BucketIterator, Iterator \n",
    "from torchtext.vocab import Vectors\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "\n",
    "from evaluation_helper import evaluation_helper\n",
    "from preprocess_helper import preprocess_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.set_num_threads(4)\n",
    "torch.set_num_interop_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem=WordNetLemmatizer()\n",
    "retokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "def tokenizer(text):\n",
    "    words=[word for word in retokenizer.tokenize(text)]\n",
    "    words=[lem.lemmatize(w) for w in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_field = Field(tokenize=tokenizer, lower=False, include_lengths=True, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = Field(tokenize='basic_english', lower=False, include_lengths=True, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_field = LabelField(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('original_text',text_field),('label', label_field)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=TabularDataset(path = 'data/WikiLarge_Train.csv',format = 'csv',fields = fields,skip_header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416768\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train_data, dev_data = training_data.split(split_ratio=0.8, random_state=random.seed(RANDOM_SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 9551\n",
      "[('the', 456951), (',', 389835), ('.', 357180), ('of', 232093), ('in', 202680), (\"'\", 167878), ('and', 167598), ('a', 156818), ('is', 131249), ('to', 99407)]\n"
     ]
    }
   ],
   "source": [
    "text_field.build_vocab(train_data, min_freq=50, vectors='charngram.100d')\n",
    " \n",
    "\n",
    "#No. of unique tokens in text\n",
    "print(\"Size of TEXT vocabulary:\",len(text_field.vocab))\n",
    "\n",
    "\n",
    "#Commonly used words\n",
    "print(text_field.vocab.freqs.most_common(10))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_field.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "\n",
    "trainIterator = BucketIterator(train_data, batch_size= 64, sort_key=lambda x: len(x.original_text),\n",
    "                            device=device, sort=False, sort_within_batch=True)\n",
    "dev_iter = BucketIterator(dev_data,  batch_size= 64, sort_key=lambda x: len(x.original_text),\n",
    "                            device=device, sort=False, sort_within_batch=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingSize = 100\n",
    "hiddenSize = 10\n",
    "dropoutRate = 0.5\n",
    "numEpochs = 5\n",
    "vocabSize = len(text_field.vocab)\n",
    "pad = 1\n",
    "unk = 0\n",
    "\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.name = model\n",
    "        self.LSTM = (model == 'LSTM' or model == 'BiLSTM')\n",
    "        self.bidir = (model == 'BiLSTM')\n",
    "        \n",
    "        self.embed = nn.Embedding(vocabSize, embeddingSize, padding_idx = pad)\n",
    "        \n",
    "        if model == 'RNN': \n",
    "            self.rnn = nn.RNN(embeddingSize, hiddenSize)\n",
    "        elif model == 'GRU': \n",
    "            self.rnn = nn.GRU(embeddingSize, hiddenSize)\n",
    "        else: \n",
    "            self.rnn = nn.LSTM(embeddingSize, hiddenSize, bidirectional=self.bidir)\n",
    "\n",
    "        self.dense = nn.Linear(hiddenSize * (2 if self.bidir else 1), 1)\n",
    "        self.dropout = nn.Dropout(dropoutRate)\n",
    "        \n",
    "    def forward(self, text, textLengths):\n",
    "        embedded = self.dropout(self.embed(text))\n",
    "        \n",
    "        packedEmbedded = nn.utils.rnn.pack_padded_sequence(embedded, textLengths, batch_first=True)\n",
    "        if self.LSTM: \n",
    "            packedOutput, (hidden, cell) = self.rnn(packedEmbedded)\n",
    "        else: \n",
    "            packedOutput, hidden = self.rnn(packedEmbedded)\n",
    "\n",
    "        output, outputLengths = nn.utils.rnn.pad_packed_sequence(packedOutput, batch_first=True)\n",
    "        if self.bidir: \n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        else: \n",
    "            hidden = hidden[0]\n",
    "\n",
    "        return self.dense(self.dropout(hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicRNN =  MyRNN(model='RNN')\n",
    "GRU =  MyRNN(model='GRU') \n",
    "LSTM = MyRNN(model='LSTM') \n",
    "biLSTM = MyRNN(model='BiLSTM') \n",
    "models = [basicRNN, GRU, LSTM, biLSTM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    if model is None:\n",
    "        continue\n",
    "    model.embed.weight.data.copy_(text_field.vocab.vectors)\n",
    "    model.embed.weight.data[unk] = torch.zeros(embeddingSize)\n",
    "    model.embed.weight.data[pad] = torch.zeros(embeddingSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def batchAccuracy(preds, targets):\n",
    "    roundedPreds = (preds >= 0)\n",
    "    return (roundedPreds == targets).sum().item() / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5210 4\n",
      "Model: RNN, Epoch: 1, Train Loss: 0.6809835895066527\n",
      "5210 4\n",
      "Model: RNN, Epoch: 2, Train Loss: 0.6614409737818072\n",
      "5210 4\n",
      "Model: RNN, Epoch: 3, Train Loss: 0.6412096203260138\n",
      "5210 4\n",
      "Model: RNN, Epoch: 4, Train Loss: 0.6305526934406808\n",
      "5210 4\n",
      "Model: RNN, Epoch: 5, Train Loss: 0.6247683097442144\n",
      "5210 4\n",
      "Model: RNN, Epoch: 6, Train Loss: 0.6216648888736678\n",
      "5210 4\n",
      "Model: RNN, Epoch: 7, Train Loss: 0.6192985341153081\n",
      "5210 4\n",
      "Model: RNN, Epoch: 8, Train Loss: 0.6214612549646344\n",
      "5210 4\n",
      "Model: RNN, Epoch: 9, Train Loss: 0.6073265984597225\n",
      "5210 4\n",
      "Model: RNN, Epoch: 10, Train Loss: 0.6031984176779892\n",
      "\n",
      "5210 4\n",
      "Model: GRU, Epoch: 1, Train Loss: 0.6205459449147079\n",
      "5210 4\n",
      "Model: GRU, Epoch: 2, Train Loss: 0.5692922777440864\n",
      "5210 4\n",
      "Model: GRU, Epoch: 3, Train Loss: 0.554219114036798\n",
      "5210 4\n",
      "Model: GRU, Epoch: 4, Train Loss: 0.5447768971295366\n",
      "5210 4\n",
      "Model: GRU, Epoch: 5, Train Loss: 0.5372544190476357\n",
      "5210 4\n",
      "Model: GRU, Epoch: 6, Train Loss: 0.5304315085288659\n",
      "5210 4\n",
      "Model: GRU, Epoch: 7, Train Loss: 0.525652111277356\n",
      "5210 4\n",
      "Model: GRU, Epoch: 8, Train Loss: 0.5208209427012821\n",
      "5210 4\n",
      "Model: GRU, Epoch: 9, Train Loss: 0.5176735998229651\n",
      "5210 4\n",
      "Model: GRU, Epoch: 10, Train Loss: 0.5136410235014391\n",
      "\n",
      "5210 4\n",
      "Model: LSTM, Epoch: 1, Train Loss: 0.6228493922073645\n",
      "5210 4\n",
      "Model: LSTM, Epoch: 2, Train Loss: 0.5692325158134074\n",
      "5210 4\n",
      "Model: LSTM, Epoch: 3, Train Loss: 0.5501579827402009\n",
      "5210 4\n",
      "Model: LSTM, Epoch: 4, Train Loss: 0.53862458321046\n",
      "5210 4\n",
      "Model: LSTM, Epoch: 5, Train Loss: 0.5302194034584196\n",
      "5210 4\n",
      "Model: LSTM, Epoch: 6, Train Loss: 0.5236845030694227\n",
      "5210 4\n",
      "Model: LSTM, Epoch: 7, Train Loss: 0.5188376377474324\n",
      "5210 4\n",
      "Model: LSTM, Epoch: 8, Train Loss: 0.5132168444041556\n",
      "5210 4\n",
      "Model: LSTM, Epoch: 9, Train Loss: 0.5085630102229668\n",
      "5210 4\n",
      "Model: LSTM, Epoch: 10, Train Loss: 0.5047554279958218\n",
      "\n",
      "5210 4\n",
      "Model: BiLSTM, Epoch: 1, Train Loss: 0.6107562937714774\n",
      "5210 4\n",
      "Model: BiLSTM, Epoch: 2, Train Loss: 0.5637608998682129\n",
      "5210 4\n",
      "Model: BiLSTM, Epoch: 3, Train Loss: 0.5453056919855028\n",
      "5210 4\n",
      "Model: BiLSTM, Epoch: 4, Train Loss: 0.5328847992328674\n",
      "5210 4\n",
      "Model: BiLSTM, Epoch: 5, Train Loss: 0.5235593109035904\n",
      "5210 4\n",
      "Model: BiLSTM, Epoch: 6, Train Loss: 0.5162757340682789\n",
      "5210 4\n",
      "Model: BiLSTM, Epoch: 7, Train Loss: 0.5105045054973087\n",
      "5210 4\n",
      "Model: BiLSTM, Epoch: 8, Train Loss: 0.5043588096458258\n",
      "5210 4\n",
      "Model: BiLSTM, Epoch: 9, Train Loss: 0.4996005316341793\n",
      "5210 4\n",
      "Model: BiLSTM, Epoch: 10, Train Loss: 0.49516129411056264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "numEpochs = 10\n",
    "\n",
    "train_time = {}\n",
    "for model in models: \n",
    "    if model is not None:\n",
    "        model.train()\n",
    "\n",
    "for model in models:\n",
    "    if model is None:\n",
    "        continue\n",
    "\n",
    "    \n",
    "\n",
    "    start_time = time.time()\n",
    "    torch.manual_seed(0)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    for epoch in range(numEpochs):\n",
    "        epochLoss = 0\n",
    "        i=0\n",
    "        j=0\n",
    "        for batch in trainIterator:\n",
    "            optimizer.zero_grad()\n",
    "            examples = [example for example in batch] #added \n",
    "            text, textLen = examples[0]\n",
    "            for n in range(len(textLen)):\n",
    "                if textLen[n] ==0:\n",
    "                    textLen[n]=1\n",
    "                    j+=1\n",
    "            predictions = model(text, textLen).squeeze(1)\n",
    "            loss = criterion(predictions, examples[1])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epochLoss += loss.item()\n",
    "            i+=1\n",
    "        print(i,j)  \n",
    "        print(f'Model: {model.name}, Epoch: {epoch + 1}, Train Loss: {epochLoss / i}')\n",
    "    train_time[model.name] = time.time() - start_time\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RNN': 971.8303790092468,\n",
       " 'GRU': 1230.415951013565,\n",
       " 'LSTM': 1226.0426499843597,\n",
       " 'BiLSTM': 1697.0781235694885}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = evaluation_helper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1303 0\n",
      "Model: RNN, Validation Accuracy: 67.68948506405337%\n",
      "1303 0\n",
      "Model: GRU, Validation Accuracy: 71.85738163409883%\n",
      "1303 0\n",
      "Model: LSTM, Validation Accuracy: 72.38085925969656%\n",
      "1303 0\n",
      "Model: BiLSTM, Validation Accuracy: 72.66856440758015%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "for model in models: \n",
    "    if model is not None:\n",
    "        model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for model in models:\n",
    "        \n",
    "        if model is None:\n",
    "            continue\n",
    "\n",
    "        accuracy = 0.0\n",
    "        i=0\n",
    "        j=0\n",
    "        for batch in dev_iter:\n",
    "            examples = [example for example in batch] #added \n",
    "            text, textLen = examples[0]\n",
    "            for n in range(len(textLen)):\n",
    "                if textLen[n] ==0:\n",
    "                    textLen[n]=1\n",
    "            predictions = model(text, textLen).squeeze(1)\n",
    "            loss = criterion(predictions, examples[1])\n",
    "            acc = batchAccuracy(predictions, examples[1])\n",
    "            accuracy += acc\n",
    "#             evaluator.evaluate(model.name, examples[1], predictions, train_time[model.name])\n",
    "            i+=1\n",
    "        print(i, j)\n",
    "        print('Model: {}, Validation Accuracy: {}%'.format(model.name, accuracy / i * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyRNN(\n",
       "  (embed): Embedding(9551, 100, padding_idx=1)\n",
       "  (rnn): LSTM(100, 10, bidirectional=True)\n",
       "  (dense): Linear(in_features=20, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
